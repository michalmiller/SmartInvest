import os, httpx
from googletrans import Translator

OLLAMA_HOST = os.getenv("OLLAMA_HOST", "https://ollama-render-kwqx.onrender.com")
MODEL_NAME  = os.getenv("OLLAMA_MODEL", "llama3")
OLLAMA_URL  = f"{OLLAMA_HOST.rstrip('/')}/api/generate"
translator = Translator()
async def call_ollama(prompt: str) -> str:
    payload = {"model": MODEL_NAME, "prompt": prompt, "stream": False}
    async with httpx.AsyncClient(timeout=120.0) as client:
        r = await client.post(OLLAMA_URL, json=payload)
        r.raise_for_status()
        return r.json().get("response", "[No response]")

async def ask_llm(question: str) -> str:
    print(" 砖 砖:", question)

    # 转专 注专转 转
    #translated = translator.translate(question, src='he', dest='en')
   # translated_question = translated.text
    # print(" 砖 转专转:", translated_question)

    # 拽专 
    response_en = await call_ollama(question)
    print(" 转砖 转:", response_en)

    # 转专 专 注专转
    #translated = translator.translate(response_en, src='en', dest='he')
    #translated_answer = translated.text
    #print(" 转砖 转专转 注专转:", response_en)

    return response_en
